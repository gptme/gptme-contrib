- {{[[TODO]]}} I want to set up a LLM-arena style benchmark combined with ideas from LLM consortium to use each model as a judge of the other's output to choose a single best answer (blindly).
    - Instead of manually testing each new LLM, could just ask the existing frontier ones if a new one is better.
    - {{[[TODO]]}} I should make a LLM consortium plugin/skill/lessons for gptme, then an experiment like the above would be trivial.
        - Also make the ActivityWatch plugin at the same time
        - Also make a image generation API supporting Google's new Nano Banana Pro (Gemini 3)
        - Plugins should probably be package-like and use uv

